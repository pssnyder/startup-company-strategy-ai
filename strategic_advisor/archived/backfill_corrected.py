#!/usr/bin/env python3
"""
Backfill Historical Data with Corrected Database Schema
Now properly capturing candidates, products, and all other game data
"""

import sys
import json
from pathlib import Path
from datetime import datetime

sys.path.append(str(Path(__file__).parent))
from correct_temporal_database import CorrectTemporalGameDatabase

def backfill_corrected_database():
    print("üöÄ Historical Backfill with CORRECTED Database Schema")
    print("="*70)
    
    # Use the corrected database
    db = CorrectTemporalGameDatabase("momentum_ai_complete.db")
    
    # Define all save file sources
    save_sources = [
        {
            'name': 'Historical Archive',
            'path': Path('game_saves/20250929_sg_momentum ai'),
            'description': 'Historical snapshots from September 29th'
        },
        {
            'name': 'Current Primary Save',
            'path': Path('C:/Users/patss/Saved Games/Startup Company/testing_v1/sg_momentum ai.json'),
            'description': 'Most recent live save file'
        },
        {
            'name': 'Current Autosave',
            'path': Path('C:/Users/patss/Saved Games/Startup Company/testing_v1/sg_momentum ai_autosave.json'),
            'description': 'Most recent autosave backup'
        },
        {
            'name': 'Processed Archive',
            'path': Path('strategic_advisor/save_files'),
            'description': 'Previously processed save files'
        }
    ]
    
    all_save_files = []
    
    # Collect all save files
    print("üîç Scanning for save files...")
    for source in save_sources:
        print(f"\nüìÇ Checking: {source['name']}")
        
        if source['path'].is_file():
            # Single file
            if source['path'].suffix == '.json':
                all_save_files.append({
                    'file': source['path'],
                    'source': source['name'],
                    'modified': source['path'].stat().st_mtime
                })
                print(f"   ‚úÖ Found: {source['path'].name} ({source['path'].stat().st_size / 1024:.1f} KB)")
        
        elif source['path'].is_dir():
            # Directory of files
            json_files = list(source['path'].glob('*.json'))
            print(f"   üìÅ Directory contains {len(json_files)} JSON files")
            
            for json_file in json_files:
                if 'sg_momentum ai' in json_file.name or 'momentum' in json_file.name.lower():
                    all_save_files.append({
                        'file': json_file,
                        'source': source['name'],
                        'modified': json_file.stat().st_mtime
                    })
                    print(f"   ‚úÖ Found: {json_file.name} ({json_file.stat().st_size / 1024:.1f} KB)")
        else:
            print(f"   ‚ùå Path not found: {source['path']}")
    
    if not all_save_files:
        print("\n‚ùå No save files found for processing!")
        return
    
    # Sort by modification time (oldest first)
    all_save_files.sort(key=lambda x: x['modified'])
    
    print(f"\nüìä Total save files found: {len(all_save_files)}")
    print("üìÖ Processing in chronological order...")
    
    # Process each save file
    processed_count = 0
    skipped_count = 0
    error_count = 0
    
    for i, save_info in enumerate(all_save_files, 1):
        save_file = save_info['file']
        source_name = save_info['source']
        modified_time = datetime.fromtimestamp(save_info['modified'])
        
        print(f"\nüìÅ [{i}/{len(all_save_files)}] Processing: {save_file.name}")
        print(f"   Source: {source_name}")
        print(f"   Modified: {modified_time}")
        print(f"   Size: {save_file.stat().st_size / 1024:.1f} KB")
        
        try:
            # Load and validate save file
            print(f"   üìñ Loading JSON data...")
            with open(save_file, 'r', encoding='utf-8') as f:
                save_data = json.load(f)
            
            # Validate it's a momentum ai save
            company_name = save_data.get('companyName', '')
            if 'momentum' not in company_name.lower():
                print(f"   ‚ùå Not a Momentum AI save file (company: {company_name}), skipping...")
                skipped_count += 1
                continue
            
            # Extract key info for preview
            game_date = save_data.get('date', 'Unknown')
            balance = save_data.get('balance', 0)
            xp = save_data.get('xp', 0)
            research_points = save_data.get('researchPoints', 0)
            candidates_count = len(save_data.get('candidates', []))
            products_count = len(save_data.get('products', []))
            
            print(f"   üìä Game Date: {game_date}")
            print(f"   üí∞ Balance: ${balance:,.2f}")
            print(f"   ‚≠ê XP: {xp:,.0f}")
            print(f"   üî¨ Research Points: {research_points}")
            print(f"   üë• Candidates: {candidates_count}")
            print(f"   üè≠ Products: {products_count}")
            
            # Process through corrected database
            print(f"   üîÑ Ingesting with COMPLETE schema...")
            save_file_id = db.ingest_save_file(save_file, save_data)
            
            print(f"   ‚úÖ Successfully processed! Database ID: {save_file_id}")
            processed_count += 1
            
        except Exception as e:
            print(f"   ‚ùå Error processing {save_file.name}: {str(e)}")
            error_count += 1
            continue
    
    print(f"\nüéØ CORRECTED Historical Backfill Complete!")
    print("="*60)
    print(f"‚úÖ Successfully processed: {processed_count} files")
    print(f"‚è≠Ô∏è Skipped (not Momentum AI): {skipped_count} files")
    print(f"‚ùå Errors: {error_count} files")
    
    if processed_count > 0:
        print(f"\nüìä Final Database Summary:")
        counts = db.get_table_counts()
        
        total_records = sum(counts.values())
        print(f"   üìä Total Records: {total_records:,}")
        
        for table_name, count in counts.items():
            if count > 0:
                print(f"   ‚úÖ {table_name}: {count:,} records")
        
        # Show sample data from key tables
        cursor = db.connection.cursor()
        
        print(f"\nüë• Sample Candidates Analysis:")
        cursor.execute("""
            SELECT employee_type_name, COUNT(*) as count, AVG(salary) as avg_salary
            FROM candidates 
            GROUP BY employee_type_name
            ORDER BY count DESC
            LIMIT 5
        """)
        for row in cursor.fetchall():
            print(f"   ‚Ä¢ {row[0]}: {row[1]} candidates, ${row[2]:,.0f} avg salary")
        
        print(f"\nüè≠ Products Timeline:")
        cursor.execute("""
            SELECT p.name, p.product_type_name, COUNT(DISTINCT sf.id) as snapshots
            FROM products p
            JOIN save_files sf ON p.save_file_id = sf.id
            GROUP BY p.name, p.product_type_name
            ORDER BY snapshots DESC
        """)
        for row in cursor.fetchall():
            print(f"   ‚Ä¢ {row[0]} ({row[1]}): {row[2]} snapshots")
    
    print(f"\nüóÑÔ∏è Database file: momentum_ai_complete.db")
    print(f"üéâ COMPLETE game save data now properly captured!")
    print(f"‚úÖ Includes: candidates, products, features, inventory, competitors, and more!")
    
    db.close()

if __name__ == "__main__":
    backfill_corrected_database()